# ğŸŒ¦ï¸ ETL Clima - Apache Airflow + OpenWeatherMap API

Proyecto ETL (Extract, Transform, Load) automatizado que ingesta datos de clima en tiempo real desde la **OpenWeatherMap API**, aplica transformaciones de normalizaciÃ³n (Flattening) y persiste los datos en formato JSON y Parquet con lÃ³gica de merge incremental (CDC - Change Data Capture).

## ğŸ“‹ Tabla de Contenidos
- [CaracterÃ­sticas](#caracterÃ­sticas)
- [Arquitectura](#arquitectura)
- [Requisitos](#requisitos)
- [InstalaciÃ³n](#instalaciÃ³n)
- [ConfiguraciÃ³n](#configuraciÃ³n)
- [Uso](#uso)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Flujo ETL](#flujo-etl)
- [Troubleshooting](#troubleshooting)

---

## âœ¨ CaracterÃ­sticas

âœ… **ExtracciÃ³n Resiliente**: Reintentos automÃ¡ticos con backoff exponencial (tenacity)  
âœ… **NormalizaciÃ³n (Flattening)**: Convierte JSON anidado a estructura plana  
âœ… **Persistencia Dual**: JSON particionado (auditorÃ­a) + Parquet maestro (anÃ¡lisis)  
âœ… **CDC Incremental**: Detecta UPDATEs e INSERTs, fusiona datos sin duplicados  
âœ… **OrquestaciÃ³n**: Apache Airflow con scheduler diario  
âœ… **Dockerizado**: Stack completo (Airflow + PostgreSQL + OpenWeatherMap)  

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenWeatherMap â”‚
â”‚      API        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     EXTRACCIÃ“N (extraer_api)    â”‚
â”‚  - Fetch batch de 7 ciudades    â”‚
â”‚  - Rate limiting Ã©tico (0.2s)   â”‚
â”‚  - Reintentos automÃ¡ticos       â”‚
â”‚  â””â”€â†’ Retorna: Lista dicts crudosâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TRANSFORMACIÃ“N (transformar)  â”‚
â”‚  - Normaliza JSON anidado       â”‚
â”‚  - Extrae campos principales    â”‚
â”‚  - Agrega timestamp             â”‚
â”‚  â””â”€â†’ Retorna: Lista dicts plana â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      CARGA (carga)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ JSON Particionado    â”‚       â”‚
â”‚  â”‚ /fecha=YYYY-MM-DD/   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚              â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Parquet + CDC Merge  â”‚       â”‚
â”‚  â”‚ climatico_maestro    â”‚       â”‚
â”‚  â”‚ (Upsert atÃ³mico)     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Requisitos

- Docker & Docker Compose (v20.10+)
- Git
- Clave API gratuita de [OpenWeatherMap](https://openweathermap.org/api)

---

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone https://github.com/Thiago-Jeager/ETL_AIRFLOW.git
cd ETL_AIRFLOW
```

### 2. Crear archivo `.env` con tus credenciales
```bash
cp .env.example .env  # Si existe, o crear manualmente
```

Contenido de `.env`:
```env
# OpenWeatherMap API
TOKEN_API=your_api_key_here

# Airflow
AIRFLOW_UID=50000
AIRFLOW_GID=50000
```

**IMPORTANTE**: Nunca commiteches `.env` al repositorio (estÃ¡ en `.gitignore`)

### 3. Construir e iniciar los contenedores
```bash
docker compose up -d
```

Esto inicia:
- **PostgreSQL** (base de datos Airflow): `postgres:5432`
- **Airflow Webserver** (UI): `http://localhost:8080`
- **Airflow Scheduler** (motor de tareas)

### 4. Acceder a Airflow
- URL: http://localhost:8080
- Usuario: `admin`
- ContraseÃ±a: `admin`

---

## âš™ï¸ ConfiguraciÃ³n

### Archivo `.env`

| Variable | DescripciÃ³n | Ejemplo |
|----------|-------------|---------|
| `TOKEN_API` | Clave de OpenWeatherMap API | `sk_test_123abc...` |
| `AIRFLOW_UID` | UID del usuario en contenedor | `50000` |
| `AIRFLOW_GID` | GID del grupo en contenedor | `50000` |

### Dockerfile

```dockerfile
FROM apache/airflow:2.7.1

RUN pip install --no-cache-dir \
    requests==2.31.0 \      # HTTP requests
    pandas==2.0.3 \         # Data manipulation
    pyarrow==14.0.1 \       # Parquet I/O
    python-dotenv==1.0.0 \  # Env variables
    tenacity                # Retry logic
```

**Paquetes instalados**:
- `requests`: Llamadas HTTP a OpenWeatherMap
- `pandas`: ManipulaciÃ³n de DataFrames
- `pyarrow`: Lectura/escritura de Parquet
- `python-dotenv`: Cargar variables de `.env`
- `tenacity`: Reintentos automÃ¡ticos con backoff

### docker-compose.yaml

**Servicios**:

#### PostgreSQL
```yaml
postgres:
  image: postgres:13
  environment:
    POSTGRES_USER: airflow
    POSTGRES_PASSWORD: airflow
    POSTGRES_DB: airflow
  volumes:
    - postgres_data:/var/lib/postgresql/data
```
Base de datos que almacena metadatos de Airflow (DAGs, ejecuciones, logs).

#### Airflow Init
```yaml
airflow-init:
  command: bash -c "airflow db init && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin"
```
Inicializa la BD y crea usuario `admin:admin`.

#### Airflow Webserver
```yaml
airflow-webserver:
  ports:
    - "8080:8080"
  command: webserver
```
Interfaz grÃ¡fica en http://localhost:8080

#### Airflow Scheduler
```yaml
airflow-scheduler:
  command: scheduler
```
Ejecuta las tareas programadas del DAG (diariamente segÃºn `schedule_interval='@daily'`).

---

## ğŸ“Š Uso

### 1. Ver el DAG en Airflow UI
1. Ir a http://localhost:8080
2. Buscar DAG: `etl_basico_clima`
3. Estado actual, ejecuciones, logs

### 2. Ejecutar manualmente
```bash
# En Airflow UI: 
# Click en el DAG â†’ Trigger DAG â†’ Confirm

# O por CLI:
docker exec -it taller\ etl-airflow-scheduler-1 airflow dags trigger -d etl_basico_clima execute
```

### 3. Ver logs
```bash
docker compose logs -f airflow-scheduler
```

### 4. Acceder a los datos generados

**JSON particionado**:
```bash
ls /opt/airflow/data/raw/fecha=2026-02-19/
# Output: datos_20260219_213235.json
```

**Parquet maestro**:
```bash
ls /opt/airflow/data/processed/
# Output: clima_maestro.parquet
```

---

## ğŸ“ Estructura del Proyecto

```
etl_airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ etl.py                    # DAG principal (Extract â†’ Transform â†’ Load)
â”‚   â”œâ”€â”€ test.py                   # DAG de prueba
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ opw_etl.py            # Cliente OpenWeatherMap + funciÃ³n merge
â”œâ”€â”€ logs/                         # Logs de Airflow (generado automÃ¡ticamente)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # JSON particionado por fecha
â”‚   â”‚   â””â”€â”€ fecha=YYYY-MM-DD/
â”‚   â”‚       â””â”€â”€ datos_HHMMSS.json
â”‚   â””â”€â”€ processed/                # Parquet maestro
â”‚       â””â”€â”€ clima_maestro.parquet
â”œâ”€â”€ plugins/                      # Plugins Airflow (vacÃ­o por defecto)
â”œâ”€â”€ .env                          # Variables de entorno (NO sincronizar)
â”œâ”€â”€ .env.example                  # Plantilla de .env
â”œâ”€â”€ .gitignore                    # Archivos a ignorar en git
â”œâ”€â”€ Dockerfile                    # Imagen personalizada de Airflow
â”œâ”€â”€ docker-compose.yaml           # OrquestaciÃ³n de servicios
â””â”€â”€ README.md                     # Este archivo
```

---

## ğŸ”„ Flujo ETL Detallado

### 1. EXTRACCIÃ“N (`extraer_api`)

**Clase**: `OpwClient` en [opw_etl.py](dags/modules/opw_etl.py)

```python
def fetch_batch_data(self, cities):
    """
    - Itera sobre lista de ciudades
    - Llama API OpenWeatherMap para cada una
    - Reintentos automÃ¡ticos (mÃ¡x 5 intentos)
    - Rate limiting: 0.2s entre llamadas
    - Retorna: lista de dicts crudos (ej: 7 ciudades)
    """
```

**Entrada**: `["London", "Tokyo", "New York", "Paris", "Berlin", "Madrid", "Sydney", "Loja"]`

**Salida**:
```python
[
  {
    "id": 2643743,
    "name": "London",
    "main": {"temp": 10.5, "humidity": 72, "pressure": 1013},
    "weather": [{"main": "Clouds"}],
    ...
  },
  ...
]
```

### 2. TRANSFORMACIÃ“N (`transformar`)

Normaliza JSON anidado a estructura plana:

```python
def transformar(raw_data):
    """
    Para cada registro crudo:
    - Extrae: id â†’ sensor_id
    - Extrae: name â†’ location
    - Aplana: main.temp â†’ temperature
    - Aplana: main.humidity â†’ humidity
    - Aplana: main.pressure â†’ pressure
    - Aplana: weather[0].main â†’ weather_condition
    - Agrega: extraction_timestamp (ISO format)
    
    Retorna: lista de dicts normalizados
    """
```

**Salida**:
```python
[
  {
    "sensor_id": 2643743,
    "location": "London",
    "temperature": 10.5,
    "humidity": 72,
    "pressure": 1013,
    "weather_condition": "Clouds",
    "extraction_timestamp": "2026-02-19T21:32:35.123456"
  },
  ...
]
```

### 3. CARGA (`carga`)

Persiste en dos formatos:

#### a) JSON Particionado (estructura Hive)
```
/opt/airflow/data/raw/fecha=2026-02-19/datos_20260219_213235.json
```
- Una carpeta por fecha (facilita limpieza/archivado)
- Archivo con timestamp (evita sobrescrituras)
- Formato: Array JSON con indentaciÃ³n (legible)

#### b) Parquet Maestro + CDC
```
/opt/airflow/data/processed/clima_maestro.parquet
```

**LÃ³gica `execute_iot_merge()`**:

1. **Bootstrap** (primera ejecuciÃ³n):
   - Si no existe el archivo
   - Crear con datos nuevos â†’ terminar

2. **CDC - Change Data Capture**:
   - Lee estado actual del master
   - Identifica `sensor_id` en datos nuevos vs master
   
3. **CategorizaciÃ³n**:
   - `UPDATES`: IDs que existen en master (datos se actualizarÃ¡n)
   - `INSERTS`: IDs nuevos (se agregarÃ¡n)

4. **Merge**:
   - Eliminar del master los IDs a actualizar
   - Concatenar: [Master Limpio] + [UPDATES] + [INSERTS]
   - Sobrescribir archivo Parquet

5. **Observabilidad**:
   ```
   âœ… MERGE COMPLETADO
   ==================================================
   ğŸ”¹ Registros actualizados (UPDATES):  7
   ğŸ”¹ Registros nuevos (INSERTS):       0
   ğŸ“ˆ Total registros en Master Final:  7
   ==================================================
   ```

---

## ğŸ” Seguridad

- **Variables sensibles**: Almacenadas en `.env` (no sincronizado)
- **API Key**: Enmascarada en logs: `sk_test_123a...cdef` âœ…
- **Base de datos**: Credenciales en docker-compose (cambiar en producciÃ³n)
- **Airflow UI**: Protegida con usuario/contraseÃ±a

---

## ğŸ› Troubleshooting

### Error: "TOKEN_API not found"
```bash
# Verificar que .env exista y tenga TOKEN_API=...
cat .env

# Reiniciar contenedores para cargar .env
docker compose down
docker compose up -d
```

### Error: "Parquet file not found" en carga
```bash
# Verificar permisos de /opt/airflow/data
docker exec airflow-scheduler ls -la /opt/airflow/data/

# Crear directorio manualmente si falta
docker exec airflow-scheduler mkdir -p /opt/airflow/data/{raw,processed}
```

### Error: "CDC merge failed - column 'sensor_id' not found"
- Verificar que transformaciÃ³n normalice correctamente
- Ver logs en http://localhost:8080 â†’ DAG â†’ Task â†’ Logs

### Limpiar datos y reiniciar
```bash
# Parar contenedores
docker compose down

# Borrar volÃºmenes (BD + datos)
docker volume rm taller_etl_postgres_data

# Reiniciar
docker compose up -d
```

---

## ğŸ“ˆ PrÃ³ximas Mejoras

- [ ] Agregar validaciÃ³n de calidad de datos (Great Expectations)
- [ ] Alerts en Slack/Email si falla extracciÃ³n
- [ ] Dashboard en Apache Superset
- [ ] Soporte para mÃºltiples fuentes de datos
- [ ] Airflow variable: lista de ciudades dinÃ¡micas
- [ ] CompresiÃ³n de Parquet (snappy)
- [ ] AuditorÃ­a: tabla de cambios (cuÃ¡ndo se actualizÃ³ quÃ©)

---

## ğŸ“ Licencia

Este proyecto es parte del curso "DiseÃ±o de procesos ETL en Data Science" - PerÃ­odo 2, MaestrÃ­a.

## ğŸ‘¤ Autor

**Santiago Loachamin**  
PerÃ­odo: 2026-02 | MaestrÃ­a en Data Science

---

## ğŸ¤ Contribuir

Para agregar features:
1. Fork del repositorio
2. Rama feature: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -am 'Add nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Pull Request

---

## ğŸ“ Soporte

- **DocumentaciÃ³n Airflow**: https://airflow.apache.org/docs/
- **OpenWeatherMap API**: https://openweathermap.org/api
- **Pandas**: https://pandas.pydata.org/docs/
